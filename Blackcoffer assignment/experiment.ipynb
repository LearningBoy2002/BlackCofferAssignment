{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bishw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bishw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_id_map = dict(zip(df['URL_ID'].astype(str) + '.txt', df['URL_ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('TitleText'):\n",
    "    os.makedirs('TitleText')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "\n",
    "    header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n",
    "    try:\n",
    "        response = requests.get(url, headers=header)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        title = soup.find('h1').get_text()\n",
    "        article = \" \".join([p.get_text() for p in soup.find_all('p')])\n",
    "\n",
    "        file_name = f'TitleText/{url_id}.txt'\n",
    "        with open(file_name, 'w', encoding='utf-8') as file:\n",
    "            file.write(title + '\\n' + article)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL_ID {url_id}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "for file in os.listdir('StopWords'):\n",
    "    with open(os.path.join('StopWords', file), 'r', encoding='ISO-8859-1') as f:\n",
    "        stop_words.update(set(f.read().splitlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = set()\n",
    "negative_words = set()\n",
    "with open('MasterDictionary/positive-words.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    positive_words.update(f.read().splitlines())\n",
    "with open('MasterDictionary/negative-words.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    negative_words.update(f.read().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(words):\n",
    "    positive_score = sum(1 for word in words if word.lower() in positive_words)\n",
    "    negative_score = sum(1 for word in words if word.lower() in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_readability(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    clean_words = [word for word in words if word.lower() not in stop_words and word.isalnum()]\n",
    "    \n",
    "    avg_sentence_length = len(clean_words) / len(sentences)\n",
    "    complex_words = [word for word in clean_words if sum(1 for char in word if char.lower() in 'aeiou') > 2]\n",
    "    percent_complex_words = len(complex_words) / len(clean_words)\n",
    "    fog_index = 0.4 * (avg_sentence_length + percent_complex_words)\n",
    "    \n",
    "    syllable_count = sum(sum(1 for char in word if char.lower() in 'aeiou') for word in clean_words)\n",
    "    avg_syllable_word_count = syllable_count / len(clean_words)\n",
    "    \n",
    "    word_count = len(clean_words)\n",
    "    avg_word_length = sum(len(word) for word in clean_words) / word_count\n",
    "    \n",
    "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.IGNORECASE))\n",
    "    \n",
    "    return avg_sentence_length, percent_complex_words, fog_index, len(complex_words), avg_syllable_word_count, word_count, avg_word_length, personal_pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for file in os.listdir('TitleText'):\n",
    "    if file.endswith('.txt'):  # Ensure we're only processing text files\n",
    "        with open(os.path.join('TitleText', file), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        words = word_tokenize(text.lower())\n",
    "        clean_words = [word for word in words if word not in stop_words and word.isalnum()]\n",
    "        \n",
    "        pos_score, neg_score, polarity, subjectivity = analyze_sentiment(clean_words)\n",
    "        avg_sent_len, perc_complex, fog, complex_count, avg_syllable, word_count, avg_word_len, pronoun_count = analyze_readability(text)\n",
    "        \n",
    "        # Use the mapping to get the correct URL_ID\n",
    "        url_id = url_id_map.get(file, None)\n",
    "        \n",
    "        if url_id is not None:\n",
    "            results.append({\n",
    "                'URL_ID': url_id,\n",
    "                'POSITIVE SCORE': pos_score,\n",
    "                'NEGATIVE SCORE': neg_score,\n",
    "                'POLARITY SCORE': polarity,\n",
    "                'SUBJECTIVITY SCORE': subjectivity,\n",
    "                'AVG SENTENCE LENGTH': avg_sent_len,\n",
    "                'PERCENTAGE OF COMPLEX WORDS': perc_complex,\n",
    "                'FOG INDEX': fog,\n",
    "                'AVG NUMBER OF WORDS PER SENTENCE': avg_sent_len,\n",
    "                'COMPLEX WORD COUNT': complex_count,\n",
    "                'WORD COUNT': word_count,\n",
    "                'SYLLABLE PER WORD': avg_syllable,\n",
    "                'PERSONAL PRONOUNS': pronoun_count,\n",
    "                'AVG WORD LENGTH': avg_word_len\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: No matching URL_ID found for file {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(df, output_df, on='URL_ID')\n",
    "final_df.to_csv('Output_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blackcoffer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
